基于Transformer的python源代码生成模型
摘要:
编程语言作为一个强大而通用的问题解决工具，驱动了各行各业的快速发展，同时也对各行业的工作人员提出了较高的技术要求。借助AI的能力实现需求指引的程序源代码生成将极大拉平非计算机专业人士使用代码的技术鸿沟，同时也可以为编程工作者提供辅助。目前主流代码生成工具都是在数十亿参数的巨大模型上训练的，模型复杂的同时具有一定安全性风险。本文实现了一个基于GPT2架构的python代码生成模型。基本数据来自于Github上的公开python源代码，在对原始数据进行了精心的处理后，测试了在低端机上训练源代码生成模型的可行性以及不同模型参数对模型性能和训练时间的影响。同时本文基于GPT模型提出了一个新的架构，意图提高模型获取上下文信息的能力。从而改善此类模型限于上下文长度无法学习到大型代码结构和复杂算法实现的问题。

Abstract:


第一章：导言
第一节：背景
在Transformer模型提出之前，自动代码生成领域成果主要集中在特定领域代码生成以及短小代码片段生成。
由微软公司分别于2011和2014年提出的Flash Fill 和 Flash Extract 项目，可以做到通过输入样例数据格式，生成代码在指定源中提取符合格式的数据项。不过这两个项目更加集中在通过对样例组进行字串分析生成正则表达式，将可能的正则表达式带入一组Domain Specific Language 中生成目标代码。
Marcel Bruch 在 2009 年提出了一个基于词频的代码补全模型。该模型通过统计公开代码中两个词出现在一起的频率，预测下文可能的输入。不过计算机代码的变量命名规则限定范围大，两个词出现在一起的几率很小，奖励矩阵十分稀疏。因此该模型在应用中有很大的局限性。
基于RNN的模型为通用代码生成工具带来了一丝曙光。Renita Priya 在2017 年的论文中分析了使用RNN生成代码的可行性。该论文分别使用RNN架构训练了生成C#，python，Java等多门主流语言的模型。均可以做到生成数行基本满足语法的代码。不过由于编成语言特性的限制，变量的定义与使用一般相隔较远，基于RNN的模型无力学习类似于此的语法语义信息。同时该类模型训练时间长，很难在庞大数据库上进行训练。

第二节：实验动机：
目前主流的代码生成模型都是在Transformer的基础上对超大规模源代码数据进行训练,下表为常见模型训练规模：
Codex 		OpenAI 							159G源代码 			12B
AlphaCode 	DeepMind 						715G源代码 			284M,1.1B,2.8B,9B,41B
PolyCoder 	Carnegie Mellon University 		254G 				2.7B
在大量的数据集上训练巨大的模型，使得这些项目在代码片段生成上都有着近乎完美的表现，不仅几乎完全不会生成语法错误的源代码片段。还有能力参考小范围内相邻代码的变量赋值,函数定义,并根据此生成符合设计逻辑的代码。但是因为目前基于Transformer模型的上下文长度限制，这些模型都很难理解大规模程序的结构信息和复杂的算法实现方式。即使是其中规模最大的AlphaCode模型，在解决实际问题时成功率也不高。其在测试样例集上最高通过率也只有29% 。而且这些项目都不是开源项目，同时其模型规模使得其不适合部署在一些计算性能较差的计算机上。
网上可以查到的小规模开源模型大多实用性不强,如下例子：
这是一个名为GPT-CC的开源项目的模型评估（在HumanEVal评测集上）:
Evaluate set : HumanEVal
Model 	pass@1 	pass@2 	pass@5 	pass@10
EleutherAI/gpt-neo 	0.12% 	0.24% 	0.61% 	1.22%
gpt-neo-125M-apps 	0.06% 	0.12% 	0.30% 	0.61%
dedup-filtered-no-resize-2048bs 	0.00% 	0.00% 	0.00% 	0.00%
1024-filtered 	0.00% 	0.00% 	0.00% 	0.00%
dedup-2048 	0.00% 	0.00% 	0.00% 	0.00%
该模型分别在gpt2和gpt3上进行训练，实测可以生成部分代码，但是在面对生成可以解决现实问题的代码时成果率就十分低。
只有一些中型的模型可以生成多行可执行代码，因此本文想分析是否有可能在本地训练出一个可以执行的代码生成工具，需要消耗多少资源，如何优化模型参数，如何优化模型架构以得到性能更好的结果。

第三节：问题分析
使用传统的人工智能模型进行源代码生成有以下难点：
1：在解决实际问题时，可能生成的程序状态空间庞大，训练时的回馈信号极其稀疏，同时程序语法语义的健壮性又十分低。不同于自然语言，对计算机程序的任何一处单个字符的改变都会显著的改变程序运行逻辑，甚至使得程序完全无法运行。也就是说，该类问题的搜索空间庞大，同时对搜索精度要求很高，这是多数机器学习模型所不擅长的部分。一般的计算机语言除了保留词汇以外，对变量等命名的词法相当宽松，这使得对源码进行词法分析和差分的难度加大，进一步加剧了训练回馈信号稀疏的问题。
2：计算机编程语言一般是结构化语言，程序员在编写源代码时，对执行逻辑的思考往往不是线性推进的。从静态语言角度来看，一个语言模型要完全理解一门编成语言，至少要可以完全理解该使用语言所写代码对应的抽象语法树。我们希望语言模型生成程序的时候可以输出对目标程序对应语法树先序遍历的结果，但这带来了一些训练模型时的困难。因为代码逻辑结构是树状的，但是代码本身是字符序列，这导致源代码中逻辑紧密相连的两个字符可能相隔很远，而相邻很近的两个字符可能在逻辑上毫无关系，如下例子中：
function 
code Block
endfunction
function与endfunction标识一个函数定义的开始与结束，一个function关键词与endfucntion关键词构成了一个完整的逻辑结构。但是因为code Block 的长度随意，function与endfunction之间可能相隔很远。无论是RNN还是Transformer模型都无法从过长的语境中提前信息，这使得此类结构化语法信息难以被学习。如果说以上例子还有可能通过调整代码语法和组织形式缩短逻辑相关词之间的距离。在如C++等静态语言中，变量的赋值，使用，函数的声明，定义，调用往往不在一起，并且还有可能分布在不同的源文件下，这为模型学习和生成带来了极大的麻烦。

第二章：实验
第一节：数据来源:
本文在Github上收集了2019-2022年间所有的100星以上的python为主语言的项目，确保这些项目是在公开协议下并且并非是其他项目的fork。在筛除其中语法不正确的源代码文件后，我们得到了1003M的纯python源代码文件。在进行了后文预处理中提到的处理办法并预留出百分之十的代码用于测试后，我们还有共13818056行python源代码。

第二节：预处理：
源码数据预处理最大的难度在于保留源代码的语法语义信息的同时将代码段拆分为多个大小合理的数据项。并在每个数据项中保留尽可能多的代码信息 无监督的开源代码包含大量无用的注释和文档字符串。同时随意的命名风格，多种不同国家的字符，不同的代码风格也会使得tokenizer生成大量的无意义token，降低模型学习效率。因此需要对源代码进行清洗。
	1：数据清洗与tokenization：
	可以观察到，直接下载的源文件中有部分是人工生成代码。多用于处理，记录坐标等数据信息。其特点为含有大量的数字，或者单行长度超过200字符串，我们首先清除调这些文件。 同时在项目结构上，我们删去了百分之八十的__init__.py,setup.py文件，减少无效信息。 我们还使用了python自带的ast工具对源代码进行语法分析，清除了其中含有语法错误的源代码。
	BPE是一种介于词级和字符级之间的Tokenize方式，可以最大化保留高频词汇的完整性的同时，还可以处理到所有的低频词汇，使之不至于都被归为UNK。源代码中有大量的自命名标识符和特殊符号，要想覆盖所有肯能的Tokens，BPE是最合适的tokenize方式，但是如果直接使用BPE 进行Tokenization，可以观察到：结果中含有大量难以解释的Tokens，这将显著降低模型的性能和可解释性。通过观察可以得知，其中多数来自大量的空白和不合理的缩进，少量来自注释和文档部分中含有的特殊字符，无意义字符串和调试信息。为此我们使用正则表达式清理了源代码中所有的注释和文档字符串。同时通过调用Black 库实现了对所有源文件的重新格式化，去除了无意义的缩进。
	由于我们需要通过tokenizer来组装用于训练的数据项（见下文），所以无法在训练数据上直接训练tokenizer。我们先抽样了100M的源代码训练tokenizer，最后在组装完成的数据库上重新训练,得到一个52000词汇量的tokenizer。
	数据隐私方面，我们收集的代码都来自开源协议下的项目。并且在训练前清理了所有的注释和文档以防其中包含私人信息。

	2：数据拆分与组装：
	GPT2 模型有每个数据项不超过1024个tokens的限制，显然我们无法直接把超过1024tokens的源代码文件直接放入一个数据项中。我们必须把复杂的源码文件进行拆分和重新组装。
	尽管Python不是可以静态编译的语言，我们仍希望每一个变量（函数）的赋值（定义）在其第一次被使用之前，并保留源代码的执行顺序。从而尽可能的保留重组之前的语法语义信息。事实上，最好每一个数据项都是语法成立，可以执行的源码字符串。同时，我门希望每一个数据项都尽可能的长，从而使得该模型有能力学习到较为复杂的算法实现和大型项目的代码结构信息。为此，我们采取了以下的源码分割策略：
	不足1024tokens的源文件直接放入一个独立的数据项。
	超过1024tokens的源文件在进行语法解析后,生成源文件对应的抽象语法树（AST)，并依序进行以下操作:
		语法树的根节点代表整个模型，其子结点可分为“Simple Statement”和“Compound Statement”，我们按出现顺序记录不同的statement。
		我们将“Simple Statement” 中的“Import Statement”放在每一个数据项的最前面（若超过150tokens则截断到不超过150tokens的部分）
		我们将“Compound Statement”中不超过500tokens的“Function Definition”和“Class Definition”依序放在未满的数据项中。
		我们将超过500tokens的“Function Definition”和“Class Definition”当作一个独立的Module处理。
		我们将剩余的“Simple Statement”和“Compound Statement”依序放在未满的数据项中。


第三节：模型与参数：(500)
这一节我将分析目前可用基础模型的优劣处，论证本文为和使用GPT2模型框架。并简介GPT2框架和Attention机制的核心部分，以便后文针对其不足指出提出改进意见。
在传统的RNN模型中，输入序列顺序检索，其中上下文信息以增量替代的方式被记录在传递的状态中,并不提供直接访问上文信息的机制。这一机制貌似可以保留上文中传递的所有信息。但由于每个节点之间传递的参数总量是固定的，即每次传递的信息总量是固定的，所以随着上文序列的增长，总有一部分的信息会在传递中被丢失。而RNN类模型的信息丢失被称为梯度消失，即表现为距离当前语境越远的上文信息保留的越少，并且信息随传递长度消减的速度十分快。这就导致了使用RNN模型生成代码时，往往只能参考相邻数行代码的信息。由第一章第三节中的讨论可知，无法捕捉到任意距离上文的信息会使得代码生成模型失去解决复杂现实问题的能力，故RNN模型不是很适合此类任务。
Transformer模型可以看作针对RNN模型无法回避的梯度消失问题的改进方案。该模型基于Attention机制，允许模型在生成新的Token时直接访问相关上（下）文中的信息。这一举措提升了上文信息的能见度，因此更为适合进行代码生成工作。多数情况下，代码生成是一个单项推进的工作，即模型只能参考上文中的代码生成下文，无法参考到下文信息。因此笔者认为基于单Decoder的GPT系列模型更加胜任此类工作。Transformer模型的核心在于如何确定上文信息的重要程度,即如何设定“Attention”，下文对此机制作简要介绍：
1：Attention机制
记输入数据项的token序列为（t1,t2...tn), word Embedding 为（e1,e2...en)。
在对输入数据项中的token进行计算时，模型根据每一个token的Word Embedding计算出Key，Value，Query三个向量，作为该token使用或参与Attention机制计算的数据。
直观上,Value为该token在模型中包含的信息，Value的计算本质上是模型对token从Wording Embedding 到模型内部信息表示方式的映射。Key值包含了对该Token的检索信息，当生成新的token时，若需使用前文信息，可通过key值决定前文token是不是要检索的对象。Query则代表了当前语境信息对前文的需求，通过计算Query和前文所有token的Key点积，可以计算出前文各tokens与当前语境的相关权重，从而协助判断前文中哪些信息对接下来的文本预测有用。
对输入信息Key,Value,Query的计算：
ki = MatrixMul(KEY,ti)
qi = 
vi = 
在预测新Token时使用的方法：

可见在预测下一个Token时,需要计算上文所有Tokens的Attention权值并进行加权求和，这限制了参与计算的上文长度。事实上GPT2理论上最多允许1023个tokens的上文参与计算。对于超过限度的上文，此类模型选择直接抛弃的方法。事实上，如果上文信息的重要程度随与当前位置距离线性衰减（正如一般日常使用的自然语言一般)，那么该模型的保留信息方式便是合理的。但如前文讨论，源代码的上文信息重要程度与位置距离并非线性关系。我们将在第三章提出一种有选择的保留超过限制的上文信息的方式，在有限的存储长度内，尽可能的保存代码中可能对下文产生影响的Tokens。例如，前文中出现“function”关键词，那么在对应的"endfunction"关键词被移除之前，该“function”关键词都应该在Attention的范围之内。不过限于计算资源，本章内仍会继续采用传统的GPT2模型进行训练，接下来简单介绍一下GPT2模型的特征。

2：GPT2简介：
GPT2模型是OpenAI团队在2019年提出的基于Transformer的通用学习模型。相对于标准的Transformer模型，GPT2是一个具备self Attention机制的单解码器模型。这里简单介绍一下GPT2模型训练以及生成的流程并定义一个在第三章需要用到的概念。
记GPT2模型单次训练（生成）的最大数据长度为：L。其中（t0 ,t1 ... tn）为已知上文，（tn+1 ... tL)为需要生成的下文序列。在生成过程中，GPT2模型采用自回归机制，逐个生成下文Token，并在生成完毕后并入已知上文中，递归的进行直到（t0 ... tn）均成为已知上文，则生成完毕。在对tn+1 进行预测时，根据上一节中Attention机制的描述，我们对上文（t0 ... tL）所有Tokens进行计算获得tn+1对词汇集中Tokens的概率分布，从而作出对tn+1个Token的预测。事实上，无论是训练模型还是生成Token时，（tn+1 ... tT)的信息都应该是不可见的。为此GPT2模型采用了掩模机制，即对未知的（或应该未知的）token，在计算Query dot key 的时候，最终值会被设定为一个十分大的负数，这样在进行softmax后，来自该token的Value 值就不会影响到模型对tn+1预测时的概率分布了。综上，在对第n+1个token进行预测时，利用了（t0 ... tn）的信息，本文称作tn+1的上文，记作context(n+1)。

3：实验用模型参数：
本次实验为了测试代码生成类工具在本地训练的可行性，我们将分别从模型大小和使用数据库大小两个方面进行更改，并且统计不同情况下训练模型的资源消耗情况和最终模型效果。
本文使用了三组不同的模型头数，层数以及参数维度，以探讨不同大小的模型对训练资源的影响以及对训练效果的影响。（这里我们使用的模型参数并非标准的GPT2-small，GPT2-medium，GPT2-large模型参数）
GPT2-small:  { "number head":8,"number layer":8,"dimensionality":592 }
GPT2-medium:  { "number head":12,"number layer":12,"dimensionality":768 }
GPT2-large:  { "number head":16,"number layer":18,"dimensionality":1024 }
同时本文准备了三种大小的数据集：
codeBaseSmall: 30M
codeBaseMedium: 50M
codeBaseLarge: 80M
以探讨数据库大小对训练资源以及成效的影响。

第四节：模型评估：(1000)
本文旨在测试训练个人代码生成工具的可行性以及提出相应优化策略，因此本部分将着力于展示不同计算情况下能否训练出模型以及模型的性能情况。限于训练模型的计算资源，本文测试的模型大小不足以学习到大型算法的实现方法。因此本文设计了一个对生成代码的语法正确性与可执行性进行评价的系统，分别测试了模型生成较长代码的语法正确率和更符合日常应用的补全当前行代码的语法正确率：
为了避免使用与训练集相同的代码评估，我们预留了百分之十的源文件用于评估生成模型的质量。我们将用于测试的源文件拆分后，将其中“Import Statement”和部分“Simple Statement”作为生成模型的输入。若该模型接下来生成的10个statements都满足语法，则视为该测试样例通过，计算所有测试样例的通过率为长代码通过率。同时我们在同一个测试集上对每个源代码文件随机抽取10到30行源代码，确保这几行可以编译通过后将最后一行代码截断，作为测试模型补全当前行的测试样例。同测试长代码生成一般测试补全当前行后的语法正确率。

1:最佳单个数据项token数：
我们在GPT2-Medium模型上，对原始代码文件进行训练。在对原始代码初始处理时我们采用了不同的策略，在对源代码进行拆分和组装时，我们改变了单个数据项中所能包含的tokens的上限和下限。分别组建了以200到300，500到600,900到1000 为单个数据项所包含tokens上下限的数据库，这里用中位数代指每一个数据库。这三个数据库中都包含30M的预处理过的源代码，作者希望通过对这三个数据组的训练得出训练资源消耗与单个数据项token数之间的规律，从而推断在资源受限情况下最佳单个数据项token数。
200-300 	11时34分钟
500-600 	11时29分钟
900-1000 	10时49分钟
通过以上实验结果可知：在训练的数据总量相同的情况下，单个数据项中平均token数越高，训练所花费的时间越高。不过上下变化幅度在10%以内，可以认为每个数据项中所包含最大tokens的数量对训练模型消耗资源影响不大。但是生成长代码的语法正确率确有显著的增加。通过观察部分生成代码可知，当单个数据项少的时候。模型无法获取上文中定义的变量，生成的代码中引用大量未定义变量，使得代码片段无法通过语法分析。同时当最近的上文中没有显著的信息时，训练单个数据项tokens少的模型倾向于重复上一段代码的动作，更加容易陷入但一重复的死循环。由以上分析可知，在计算资源不足的情况下，仍应该使得训练数据集单个数据项包含尽可能多的代码信息。本文中接下来所有的模型都会在900-1000tokens的数据集上训练。

2：数据集大小


3：模型大小



第五节：模型部署与应用：(1000)

第三章：模型优化讨论：(1000)
1：关于突破单个数据项长度限制的讨论
GPT2模型对单个数据项有最长1024tokens的限制，在实际操作中作者发现：很多复杂的算法很难在1024个token限制内进行描述。再加上为了满足代码的定义连贯性和完整性，每一个数据项中都有相当一部分是全局定义和引用到的函数，类定义。这导致我们的模型几乎没有办法学习到复杂的算法概念和代码结构。当一个项目需要直接引用到超过1024个tokens之前定义的函数和变量时，该模型就束手无策了。本章中我将分析这一问题的关键原因，并试图提出一种基于GPT2模型进行改良的架构以减轻这一限制的对生成代码复杂度约束。
由第二章第三节的分析可知，对tn+1的token进行预测时，需要综合前文context（n+1）的信息，而这是通过对context（n+1）中的n个tokens分别计算Attention做到的。也就是所，保留的前文信息越多，模型越大，计算资源要求越高。故模型所保留的前文信息一定是有限的。记为L。由第一章第三节问题分析部分可知，基于Transformer模型不始于源代码生成的主要原因在于：非线性的代码逻辑结构与语法结构使得两行代码之间的逻辑距离和两行代码在源文件之中的字符距离不必然相关。而基于Transformer模型在对保存上下文取舍时的唯一标准是字符距离。正是这种不匹配使得此类模型在生成代码文件时的表现不如处理自然语言。为了使这种不匹配更加容易理解，下文对其进行定量描述。
现记要生成的目标代码为Target Code：（m1 , m2 ... mT)
目前处理的代码片段为：（t1，t2，t3 ... tL）为 Target Code 的子序列。
给出以下定义：

两字符（mi，mj）之间的逻辑关系： R（mi，mj） (j > i)
由第二章第三节的分析可知：Attention 机制中，token mi 对生成token mj 的影响由计算 （Query（mi-1） * Key（mj））得出。这里可以认为若模型的上文长度任意大，并且模型足够好。（Query（mi-1） * Key（mj））可以反映mi和mj之间的逻辑关系，即
R（mi，mj） = （Query（mi-1）* Key（mj））

两字符 (mi, mj) 之间的字符距离： D（mi，mj） =  |j - i| （j > i）

对给定字符mj，生成字符的上文: context(mi) : { mi | mi 属于 Target Code i < j 并且 mi 参与到mj的概率分布计算中 }

在生成字符mj时，前文字符mi的重要程度：importancej(i)
这个变量意在反映上文内容对下文的重要程度，在对上文内容存储能力有限的情况下，可以根据importance的分布决定保留和抛弃那些上文,即:
context（mn） = { mn | any mi in context(n) any mj not in context(n) importance(i) > importance(j) }

假设目前处理片段中token：tn 对应在Traget Code 的token 为 mi。
在理想的模型中：
importancej(i) = R（mi，mj） = （Query（mi-1）* Key（mj））
context（mn） = { mn | any mi in context(n) any mj not in context(n) importance(i) > importance(j) }
              = { mn | any mi in context(n) any mj not in context(n) R(mi,mn) > R(mj,mn) }

在GPT模型中：
context（tn） = （t1,t2...tn-1) = (mi-n+2 .... mi-1 )
由context（tn）可以看作是前文中importance最高的T个值组成的集合，可认为GPT默认importance值由D（mi，mj）决定，如下：
importancej（i） = { 0 if D(mi,mj) > L
					  1 if D(mi,mj) < L}
这显然是一种不合理的定义方式，也导致了当逻辑距离与文本距离非线性相关时的上下文选取不当的问题。为此，我将在下一节讨论一种新的选取上下文信息的方式，力图最大程度的使得每个token生成时都可以访问到相关的上文信息。

首先我们引入一个储存机制，有选择的储存一些过往数据项的信息。这个存储机制中存储的数据应该是针对每个Python源码文件（项目）的，即:在使用该模型补全或生成一个python文件（项目）时，存储器中仅存有当前项目上文中出现过的信息，在处理下一个python文件（项目)时，应重置该存储器。
该存储器为一个大小为L/2的缓存，在进行一个新的源代码文件生成的开始时，该缓存被置空。
接下来我们引进相应的查询方式，以期在生成下一个token时可以利用Attention机制和查询机制访问到相关的历史信息。
